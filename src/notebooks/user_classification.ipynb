{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Effects of Social Media Bots on the Cryptomarket: User Classification\n",
    "\n",
    "*By Daniel Deutsch*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from datetime import datetime\n",
    "\n",
    "import emoji\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from IPython.display import clear_output\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from wordcloud import STOPWORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ignore warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Matplotlib styles\n",
    "plt.style.use('ggplot')\n",
    "plt.rcParams.update({\n",
    "    'figure.figsize': (15, 6),\n",
    "    'axes.prop_cycle': plt.cycler(color=['#4C72B0', '#C44E52', '#55A868', '#8172B2', '#CCB974', '#64B5CD']),\n",
    "    'axes.facecolor': '#EAEAF2'\n",
    "})\n",
    "\n",
    "# Constants\n",
    "START_DATE = datetime(2019, 6, 1)\n",
    "END_DATE = datetime(2022, 6, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enhance Users Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_users = pd.read_csv(\"./datasets/processed/users.csv.gz\", index_col=0, parse_dates=['join_date'])\n",
    "df_twits = pd.read_csv(\"./datasets/processed/twits.csv.gz\", index_col=0, parse_dates=['date'], low_memory=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Existing Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# id\n",
    "df_users['id'] = df_users['id']\n",
    "\n",
    "# username\n",
    "df_users['username'] = df_users['username']\n",
    "\n",
    "# name\n",
    "df_users['name'] = df_users['name']\n",
    "\n",
    "# avatar_url\n",
    "df_users['avatar_url'] = (df_users['avatar_url'] != \"http://avatars.stocktwits.com/images/default_avatar_thumb.jpg\").astype(int)\n",
    "\n",
    "# avatar_url_ssl\n",
    "del df_users['avatar_url_ssl']\n",
    "\n",
    "# join_date\n",
    "df_users['join_date'] = df_users['join_date']\n",
    "\n",
    "# official\n",
    "df_users['official'] = df_users['official'].astype(int)\n",
    "\n",
    "# identity\n",
    "del df_users['identity']\n",
    "\n",
    "# classification\n",
    "df_users['suggested'] = df_users['classification'].str.contains('suggested').astype(int)\n",
    "df_users['verified'] = df_users['classification'].str.contains('verified').astype(int)\n",
    "del df_users['classification']\n",
    "\n",
    "# home_country\n",
    "df_users['from_us'] = (df_users['home_country'] == \"US\").astype(int)\n",
    "df_users['from_ca'] = (df_users['home_country'] == \"CA\").astype(int)\n",
    "df_users['from_in'] = (df_users['home_country'] == \"IN\").astype(int)\n",
    "del df_users['home_country']\n",
    "\n",
    "# search_country\n",
    "del df_users['search_country']\n",
    "\n",
    "# followers\n",
    "df_users['followers'] = df_users['followers'].clip(lower=0)\n",
    "\n",
    "# following\n",
    "df_users['following'] = df_users['following']\n",
    "\n",
    "# ideas\n",
    "df_users['ideas'] = df_users['ideas']\n",
    "\n",
    "# watchlist_stocks_count\n",
    "df_users['watchlist_stocks_count'] = df_users['watchlist_stocks_count']\n",
    "\n",
    "# like_count\n",
    "df_users['like_count'] = df_users['like_count']\n",
    "\n",
    "# plus_tier\n",
    "df_users['plus_tier'] = (~df_users['plus_tier'].isna()).astype(int)\n",
    "\n",
    "# premium_room\n",
    "df_users['premium_room'] = (~df_users['premium_room'].isna()).astype(int)\n",
    "\n",
    "# trade_app\n",
    "df_users['trade_app'] = df_users['trade_app'].astype(int)\n",
    "\n",
    "# trade_status\n",
    "df_users['trade_status'] = (~df_users['trade_status'].isna()).astype(int)\n",
    "\n",
    "# portfolio_waitlist\n",
    "del df_users['portfolio_waitlist']\n",
    "\n",
    "# portfolio_status\n",
    "df_users['portfolio_status'] = (~df_users['portfolio_status'].isna()).astype(int)\n",
    "\n",
    "# portfolio\n",
    "del df_users['portfolio']\n",
    "\n",
    "# n_twits\n",
    "df_users['n_twits'] = df_users['n_twits']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_active_days\n",
    "df_users['n_active_days'] = df_users['join_date'].apply(lambda x: ( END_DATE - x ).days )\n",
    "\n",
    "# n_active_days_clipped\n",
    "df_users['n_active_days_clipped'] = df_users['join_date'].apply(lambda x: ( END_DATE - max(START_DATE, x) ).days )\n",
    "\n",
    "# twit_freq\n",
    "df_users['twit_freq'] = df_users['n_twits']/df_users['n_active_days_clipped']\n",
    "\n",
    "# idea_freq\n",
    "df_users['idea_freq'] = df_users['n_twits']/df_users['n_active_days']\n",
    "\n",
    "# Create the columns in the dataset\n",
    "df_users['url_rate'] = 0\n",
    "df_users['n_words_per_twit'] = 0\n",
    "df_users['n_assets_per_twit'] = 0\n",
    "df_users['n_emojis_per_twit'] = 0\n",
    "df_users['n_stopwords_per_twit'] = 0\n",
    "df_users['avg_twit_similarity'] = 0\n",
    "df_users['n_commas_per_twit'] = 0\n",
    "df_users['n_points_per_twit'] = 0\n",
    "df_users['n_semicolons_per_twit'] = 0\n",
    "df_users['n_exclamations_per_twit'] = 0\n",
    "df_users['n_quotes_per_twit'] = 0\n",
    "df_users['n_oparentheses_per_twit'] = 0\n",
    "df_users['n_cparentheses_per_twit'] = 0\n",
    "\n",
    "\n",
    "i, n = 0, df_twits.groupby('user.id').ngroups\n",
    "for user_id, twits in df_twits.groupby('user.id'):\n",
    "\n",
    "    print(f\"\\r{i}/{n-1}\", end=\"\")\n",
    "    i += 1\n",
    "    \n",
    "    # Drop duplicated twits\n",
    "    twits = twits.drop_duplicates(subset=['id'], ignore_index=True)\n",
    "    twits = twits.dropna(subset=['text'])\n",
    "\n",
    "    # Obtains general params\n",
    "    n_twits = max(twits.shape[0], 1)\n",
    "    n_urls = twits['text'].str.findall(r\"[A-Za-z0-9]+://[A-Za-z0-9%-_]+(/[A-Za-z0-9%-_])*(#|\\\\?)[A-Za-z0-9%-_&=]*\").apply(len).sum()\n",
    "    n_words = twits['text'].str.split(\" \").apply(len).sum()\n",
    "    n_assets = twits['text'].str.split(r\"\\$([a-zA-Z]+)\\.x\").apply(len).sum()\n",
    "    n_emojis = twits['text'].apply(lambda x: len([c for c in x if c in emoji.UNICODE_EMOJI['en'] ])).sum()\n",
    "    n_stopwords = twits['text'].apply(lambda x: len(set(x.split()) & STOPWORDS)).sum()\n",
    "    n_commas = twits['text'].str.count(\",\").sum()\n",
    "    n_points = twits['text'].str.count(\".\").sum()\n",
    "    n_semicolons = twits['text'].str.count(\";\").sum()\n",
    "    n_exclamations = twits['text'].str.count(\"!\").sum()\n",
    "    n_quotes = twits['text'].str.count(\"\\\"\").sum()\n",
    "    n_oparentheses = twits['text'].str.count(\"\\(\").sum()\n",
    "    n_cparentheses = twits['text'].str.count(\"\\)\").sum()\n",
    "\n",
    "    # url_rate\n",
    "    df_users.loc[df_users['id'] == user_id, 'url_rate'] = n_urls/n_twits\n",
    "\n",
    "    # n_words_per_twit\n",
    "    df_users.loc[df_users['id'] == user_id, 'n_words_per_twit'] = n_words/n_twits\n",
    "\n",
    "    # n_assets_per_twit\n",
    "    df_users.loc[df_users['id'] == user_id, 'n_assets_per_twit'] = n_assets/n_twits\n",
    "\n",
    "    # n_emojis_per_twit\n",
    "    df_users.loc[df_users['id'] == user_id, 'n_emojis_per_twit'] = n_emojis/n_twits\n",
    "\n",
    "    # n_stopwords_per_twit\n",
    "    df_users.loc[df_users['id'] == user_id, 'n_stopwords_per_twit'] = n_stopwords/n_twits\n",
    "\n",
    "    # avg_twit_similarity\n",
    "    try:\n",
    "        tfidf = TfidfVectorizer(stop_words='english').fit_transform(twits['text'])\n",
    "        similarity = cosine_similarity(tfidf, tfidf)\n",
    "        df_users.loc[df_users['id'] == user_id, 'avg_twit_similarity'] = similarity[np.triu_indices_from(similarity, k=1)].mean()\n",
    "    except:\n",
    "        df_users.loc[df_users['id'] == user_id, 'avg_twit_similarity'] = pd.NA\n",
    "    \n",
    "    # n_commas_per_twit\n",
    "    df_users.loc[df_users['id'] == user_id, 'n_commas_per_twit'] = n_commas/n_twits\n",
    "\n",
    "    # n_points_per_twit\n",
    "    df_users.loc[df_users['id'] == user_id, 'n_points_per_twit'] = n_points/n_twits\n",
    "\n",
    "    # n_semicolons_per_twit\n",
    "    df_users.loc[df_users['id'] == user_id, 'n_semicolons_per_twit'] = n_semicolons/n_twits\n",
    "\n",
    "    # n_exclamations_per_twit\n",
    "    df_users.loc[df_users['id'] == user_id, 'n_exclamations_per_twit'] = n_exclamations/n_twits\n",
    "\n",
    "    # n_quotes_per_twit\n",
    "    df_users.loc[df_users['id'] == user_id, 'n_quotes_per_twit'] = n_quotes/n_twits\n",
    "\n",
    "    # n_oparentheses_per_twit\n",
    "    df_users.loc[df_users['id'] == user_id, 'n_oparentheses_per_twit'] = n_oparentheses/n_twits\n",
    "\n",
    "    # n_cparentheses_per_twit\n",
    "    df_users.loc[df_users['id'] == user_id, 'n_cparentheses_per_twit'] = n_cparentheses/n_twits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Enhanced Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_users.to_csv(\"./datasets/enhanced/users1.csv.gz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variational Autoencoder Bot Detection Model\n",
    "\n",
    "Another popular approach to anomaly detection that has gained a lot of traction as deep learning became more widely available is based on reconstruction methods. The underlying idea is based on the assumption that if a model can learn a function that compresses and reconstructs normal data, then it will fail to do so when encountered with anomalous data because its function was only trained on normal data. The failure to reconstruct data or, more accurately, the range of the reconstruction error that it entails, can therefore signal the presence of anomalous data.\n",
    "\n",
    "An autoencoder is a deep learning model that is usually based on two main components: an encoder that learns a lower-dimensional representation of input data, and a decoder that tries to reproduce the input data in its original dimension using the lower-dimensional representation generated by the encoder. The idea underlying this architecture is quite similar to that of image compression: a well-trained encoder learns to encode the input data in such a way that will capture the most important information it contains and which will therefore be sufficient (or as close as possible to be sufficient) to reproduce it by the decoder.\n",
    "\n",
    "In a VAE, the encoder similarly learns a function that takes as its input a vector of size n. However, instead of learning how to generate a latent vector that the decoder function can reproduce, as traditional AEs do, a VAE learns to generate two vectors (of size m) that represent the parameters (mean and variance) of a distribution from which the latent vector is sampled, and which the decoder function can transform back to the original input vector. Simply put, while the AE’s learning task is to learn a function that will transform data into a latent vector that a decoder can easily reproduce, the VAE’s learning task is to learn a function that will generate parameters of distributions from which a latent vector that a decoder can easily reproduce can be sampled."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_users = pd.read_csv(\"./datasets/enhanced/users1.csv.gz\", index_col=0, parse_dates=['join_date'])\n",
    "df_users.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.isna(df_users['avg_twit_similarity']).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reads the enhanced dataset\n",
    "df_users = pd.read_csv(\"./datasets/enhanced/users1.csv.gz\", index_col=0, parse_dates=['join_date'])\n",
    "\n",
    "# Removes rows without data\n",
    "df_users = df_users.dropna()\n",
    "\n",
    "# Selects only the used features\n",
    "X = df_users.drop(['id', 'username', 'name', 'join_date'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "X = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(args):\n",
    "    z_mean, z_log_var = args\n",
    "    batch = tf.keras.backend.shape(z_mean)[0]\n",
    "    dim = tf.keras.backend.int_shape(z_mean)[1]\n",
    "    epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
    "    return z_mean + tf.keras.backend.exp(0.5 * z_log_var) * epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sets the encoder's input\n",
    "encoder_input = tf.keras.layers.Input(shape=(X.shape[1],), name=\"encoder_input\")\n",
    "\n",
    "# Obtains the encoder's output\n",
    "encoder_output = tf.keras.layers.Dense(X.shape[1]//2, activation='relu')(encoder_input)\n",
    "z_mean = tf.keras.layers.Dense(X.shape[1]//3, name=\"z_mean\")(encoder_output)\n",
    "z_log_var = tf.keras.layers.Dense(X.shape[1]//3, name=\"z_log_var\")(encoder_output)\n",
    "encoder_output = tf.keras.layers.Lambda(sample, output_shape=(X.shape[1]//3,), name=\"latent_space\")([z_mean, z_log_var])\n",
    "\n",
    "# Builds the encoder\n",
    "encoder = tf.keras.Model(inputs=encoder_input, outputs=encoder_output, name=\"encoder\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sets the decoder's input\n",
    "decoder_input = tf.keras.layers.Input(shape=(X.shape[1]//3,), name=\"decoder_input\")\n",
    "\n",
    "# Obtains the decoder's output\n",
    "decoder_output = tf.keras.layers.Dense(X.shape[1]//2, activation='relu')(decoder_input)\n",
    "decoder_output = tf.keras.layers.Dense(X.shape[1], activation='sigmoid')(decoder_output)\n",
    "\n",
    "# Builds the decoder\n",
    "decoder = tf.keras.Model(decoder_input, decoder_output, name=\"decoder\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Variational Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sets the vae's input\n",
    "vae_input = encoder_input\n",
    "\n",
    "# Obtains the vae's output\n",
    "vae_output = encoder(vae_input)\n",
    "vae_output = decoder(vae_output)\n",
    "\n",
    "# Builds the vae model\n",
    "model = tf.keras.Model(encoder_input, vae_output, name=\"vae\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defines the vae's optimizer\n",
    "learning_rate = 1e-4\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate, clipvalue=0.5)\n",
    "\n",
    "# Defines the vae's loss function\n",
    "def vae_loss(x, x_decoded):\n",
    "    reconstruction_loss = tf.keras.backend.sum(tf.keras.backend.square(x - x_decoded))\n",
    "    kl_loss = -0.5 * (1 + z_log_var - tf.keras.backend.square(z_mean) - tf.keras.backend.exp(z_log_var))\n",
    "    kl_loss = tf.reduce_mean(tf.reduce_sum(kl_loss, axis=-1))\n",
    "    total_loss = tf.keras.backend.mean(reconstruction_loss + kl_loss)\n",
    "    return reconstruction_loss\n",
    "\n",
    "# Compiles the vae model\n",
    "model.compile(optimizer, loss=vae_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sets training params\n",
    "epochs = 500\n",
    "batch_size = 1024\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.ModelCheckpoint(\"./models/user_cls/weights.h5\", monitor='loss', save_freq='epoch', save_best_only=True),\n",
    "    tf.keras.callbacks.EarlyStopping(monitor='loss', min_delta=0.05, patience=5, restore_best_weights=True),\n",
    "    tf.keras.callbacks.CSVLogger(\"./models/user_cls/history.csv\", separator=',', append=True)\n",
    "]\n",
    "\n",
    "# Trains the model\n",
    "history = model.fit(\n",
    "    x=X, y=X,\n",
    "    epochs=epochs,\n",
    "    batch_size=batch_size,\n",
    "    callbacks=callbacks,\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "# Saves the model\n",
    "model.save(\"./models/user_cls\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Predicting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loads the model's weights\n",
    "model.load_weights(\"./models/user_cls/weights.h5\")\n",
    "\n",
    "# Obtains the decoded variables\n",
    "X_decoded = model.predict(X)\n",
    "\n",
    "# Merges the original and the decoded sets\n",
    "X_full = np.hstack([X, X_decoded])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K Nearest Neighbors (KNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Builds the KNN model with k=6\n",
    "nbrs = NearestNeighbors(n_neighbors=6)\n",
    "\n",
    "# Fits the model\n",
    "nbrs.fit(X_full)\n",
    "\n",
    "# Obtains the neighbors' distances\n",
    "dists, idx = nbrs.kneighbors(X_full)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining Threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "# Matplotlib styles\n",
    "plt.style.use('ggplot')\n",
    "plt.rcParams.update({\n",
    "    'figure.figsize': (15, 7),\n",
    "    'axes.prop_cycle': plt.cycler(color=['#4C72B0', '#C44E52', '#55A868', '#8172B2', '#CCB974', '#64B5CD']),\n",
    "    'axes.labelsize': 22,\n",
    "    'axes.titlesize': 24,\n",
    "    'xtick.labelsize': 14,\n",
    "    'ytick.labelsize': 14,\n",
    "    'legend.fontsize': 16,\n",
    "    'legend.title_fontsize': 16,\n",
    "    'axes.labelpad': 10,\n",
    "    'axes.facecolor': '#EAEAF2'\n",
    "})\n",
    "\n",
    "# Saving params\n",
    "saving_folder = \"./latex\"\n",
    "saving_format = 'png'\n",
    "dpi = 100\n",
    "\n",
    "ax = sns.histplot(dists.mean(axis=1), bins=500, log_scale=(False, True), kde=False)\n",
    "ax.set_xlabel(\"Average Distance\")\n",
    "ax.set_ylabel(\"User Count\")\n",
    "plt.savefig(f\"{saving_folder}/imgs/distance_distribution.{saving_format}\", format=saving_format, dpi=dpi, bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sets a threshold\n",
    "thold = 0.09\n",
    "\n",
    "# Gets the percentage of anomalies in the dataset\n",
    "n_anomalies = (dists.mean(axis=1) > thold).astype(int).sum()\n",
    "pct_anomalies = n_anomalies/dists.shape[0]\n",
    "\n",
    "# Plots the Anomaly Region\n",
    "plt.scatter(range(dists.shape[0]), dists.mean(axis=1), s=3)\n",
    "plt.axhspan(thold, max(dists.mean(axis=1)), alpha=0.2, color='r')\n",
    "plt.title(f\"Anomaly Region ({100*pct_anomalies:.2f}% anomalies)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifying Users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_users['type'] = \"Human\"\n",
    "df_users.loc[dists.mean(axis=1) > thold, 'type'] = \"Bot\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_users.to_csv(\"./datasets/enhanced/users2.csv.gz\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
