{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Master Thesis: Data Visualization\n",
    "\n",
    "*By Daniel Deutsch*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import re\n",
    "import string\n",
    "import textwrap\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "from sklearn.metrics import (classification_report, confusion_matrix,\n",
    "                             roc_auc_score, roc_curve)\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from statsmodels.tsa.api import VAR\n",
    "from statsmodels.tsa.stattools import adfuller, grangercausalitytests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ignore warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Matplotlib styles\n",
    "plt.style.use('ggplot')\n",
    "plt.rcParams.update({\n",
    "    'figure.figsize': (15, 7),\n",
    "    'axes.prop_cycle': plt.cycler(color=['#4C72B0', '#C44E52', '#55A868', '#8172B2', '#CCB974', '#64B5CD']),\n",
    "    'axes.labelsize': 22,\n",
    "    'axes.titlesize': 24,\n",
    "    'xtick.labelsize': 14,\n",
    "    'ytick.labelsize': 14,\n",
    "    'legend.fontsize': 16,\n",
    "    'legend.title_fontsize': 16,\n",
    "    'axes.labelpad': 10,\n",
    "    'axes.facecolor': '#EAEAF2'\n",
    "})\n",
    "\n",
    "# Saving params\n",
    "saving_folder = \"./latex\"\n",
    "saving_format = 'png'\n",
    "dpi = 100\n",
    "\n",
    "def grangers_causation_matrix(data, maxlag, test='ssr_chi2test', verbose=False):    \n",
    "    \"\"\"Check Granger Causality of all possible combinations of the Time series.\n",
    "    The rows are the response variable, columns are predictors. The values in the table \n",
    "    are the P-Values. P-Values lesser than the significance level (0.05), implies \n",
    "    the Null Hypothesis that the coefficients of the corresponding past values is \n",
    "    zero, that is, the X does not cause Y can be rejected.\n",
    "\n",
    "    data      : pandas dataframe containing the time series variables\n",
    "    variables : list containing names of the time series variables.\n",
    "    \"\"\"\n",
    "    variables = data.columns\n",
    "    df = pd.DataFrame(np.zeros((len(variables), len(variables))), columns=variables, index=variables)\n",
    "    for c in df.columns:\n",
    "        for r in df.index:\n",
    "            test_result = grangercausalitytests(data[[r, c]], maxlag=maxlag, verbose=False)\n",
    "            p_values = [round(test_result[i+1][0][test][1],4) for i in range(maxlag)]\n",
    "            if verbose: print(f'Y = {r}, X = {c}, P Values = {p_values}')\n",
    "            min_p_value = np.min(p_values)\n",
    "            df.loc[r, c] = min_p_value\n",
    "    df.columns = [var + ' X' for var in variables]\n",
    "    df.index = [var + ' Y' for var in variables]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hist = pd.read_csv(\"./models/user_cls/history.csv\")\n",
    "\n",
    "plt.plot(df_hist['epoch'], df_hist['loss'])\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.savefig(f\"{saving_folder}/imgs/user_cls_loss.{saving_format}\", format=saving_format, dpi=dpi, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tmp = pd.DataFrame(df_raw.dtypes, columns=['DType']).reset_index().rename(columns={'index': 'Name'})\n",
    "# df_tmp['Example'] = df_raw[df_raw['id'] == 467121451].T[21].tolist()\n",
    "# df_tmp['Example'].iloc[3] = \"{'id': USER_ID, 'username': 'USERNAME', 'name': 'NAME', 'avatar_url': 'HTTPURL', 'avatar_url_ssl': 'HTTPURL', 'join_date': '2017-09-19', 'official': False, 'identity': 'User', 'classification': ['verified'], 'home_country': 'US', 'search_country': 'US', 'followers': 54, 'following': 132, 'ideas': 3251, 'watchlist_stocks_count': 285, 'like_count': 533, 'plus_tier': 'year', 'premium_room': '', 'trade_app': False, 'portfolio': 'private', 'portfolio_status': 'private', 'trade_status': 'PENDING_MFA'}\"\n",
    "# df_tmp['Example'].iloc[10] = \"{'total': 3, 'user_ids': [USER_ID1, USER_ID2, USER_ID3]}\"\n",
    "\n",
    "df_tmp.style.to_latex(\n",
    "    f\"{saving_folder}/tables/processed_user_data_structure.tex\",\n",
    "    caption=\"Structure of processed users\",\n",
    "    label=\"table:processed_user_data_structure\",\n",
    "    position_float='centering',\n",
    "    position='H',\n",
    "    hrules=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tmp['Example'].iloc[10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cryptomap Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cryptomap = pd.read_csv(\"./datasets/raw/cryptomap.csv.gz\", index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OHLCV Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ohlcv = pd.read_csv(\"./datasets/processed/ohlcv.csv.gz\", index_col=0, parse_dates=['date'], low_memory=False)\n",
    "df_ohlcv = df_ohlcv[['date', 'base_asset', 'price']].set_index(['base_asset', 'date']).unstack('base_asset')['price']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BTC and ETH Over Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(df_ohlcv.index, df_ohlcv['BTC'], label='BTC')\n",
    "plt.plot(df_ohlcv.index, df_ohlcv['ETH'], label='ETH')\n",
    "plt.ylabel(\"Hourly Close Price (USD)\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.legend()\n",
    "plt.savefig(f\"{saving_folder}/imgs/close_price_btc_eth.{saving_format}\", format=saving_format, dpi=dpi, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing Data Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.heatmap(df_ohlcv.isna(), cmap=sns.cm.rocket_r, cbar=False)\n",
    "ax.set_yticklabels([ ylabel.get_text().split('T')[0] for ylabel in ax.get_yticklabels() ])\n",
    "ax.set_xlabel(\"Crypto Asset\")\n",
    "ax.set_ylabel(\"Date\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.savefig(f\"{saving_folder}/imgs/ohlcv_missing_data.{saving_format}\", format=saving_format, dpi=dpi, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Users Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_users = pd.read_csv(\"./datasets/enhanced/users2.csv.gz\", index_col=0, parse_dates=['join_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_users.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, axs = plt.subplots(3, 2, figsize=(25, 18))\n",
    "\n",
    "sns.histplot(df_users['url_rate'], bins=100, log_scale=(False, True), kde=False, ax=axs[0, 0])\n",
    "axs[0, 0].set_xlabel(\"URL Rate\")\n",
    "axs[0, 0].set_ylabel(\"User Count\")\n",
    "\n",
    "sns.histplot(df_users['n_words_per_twit'], bins=100, log_scale=(False, True), kde=False, ax=axs[0, 1])\n",
    "axs[0, 1].set_xlabel(\"Number of Words per Twit\")\n",
    "axs[0, 1].set_ylabel(\"User Count\")\n",
    "\n",
    "sns.histplot(df_users['n_assets_per_twit'], bins=100, log_scale=(False, True), kde=False, ax=axs[1, 0])\n",
    "axs[1, 0].set_xlabel(\"Number of Assets per Twit\")\n",
    "axs[1, 0].set_ylabel(\"User Count\")\n",
    "\n",
    "sns.histplot(df_users['n_emojis_per_twit'], bins=100, log_scale=(False, True), kde=False, ax=axs[1, 1])\n",
    "axs[1, 1].set_xlabel(\"Number of Emojis per Twit\")\n",
    "axs[1, 1].set_ylabel(\"User Count\")\n",
    "\n",
    "sns.histplot(df_users['n_stopwords_per_twit'], bins=100, log_scale=(False, True), kde=False, ax=axs[2, 0])\n",
    "axs[2, 0].set_xlabel(\"Number of Stopwords per Twit\")\n",
    "axs[2, 0].set_ylabel(\"User Count\")\n",
    "\n",
    "sns.histplot(df_users['avg_twit_similarity'], bins=100, log_scale=(False, True), kde=False, ax=axs[2, 1])\n",
    "axs[2, 1].set_xlabel(\"Average Twit Similarity\")\n",
    "axs[2, 1].set_ylabel(\"User Count\")\n",
    "\n",
    "plt.savefig(f\"{saving_folder}/imgs/users_distributions.{saving_format}\", format=saving_format, dpi=dpi, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tmp = df_users.rename(columns={'is_bot': 'Number of Samples'})\n",
    "df_tmp['Number of Samples'] = df_tmp['Number of Samples'].replace({True: \"Bot\", False: \"Human\"})\n",
    "df_tmp = df_tmp['Number of Samples'].value_counts().to_frame()\n",
    "df_tmp = df_tmp.pivot_table(\n",
    "    index=df_tmp.index,\n",
    "    margins=True,\n",
    "    margins_name='TOTAL',\n",
    "    aggfunc=np.sum\n",
    ")\n",
    "df_tmp = df_tmp.style.format('{:,}')\n",
    "\n",
    "df_tmp.to_latex(\n",
    "    f\"{saving_folder}/tables/user_types_count.tex\",\n",
    "    caption=\"Number of users per user type\",\n",
    "    label=\"table:user_types_count\",\n",
    "    position_float='centering',\n",
    "    position='H',\n",
    "    hrules=True\n",
    ")\n",
    "df_tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twits Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_twits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_twits = pd.read_csv(\"./datasets/classified/twits.csv.gz\", index_col=0, parse_dates=['date'], low_memory=False)\n",
    "df_twits['date'] = df_twits['date'].dt.tz_localize(None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_twits = pd.merge(df_twits, df_users.add_prefix('user.'), on='user.id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_twits = df_twits.dropna(subset=['user.type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_twits.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.heatmap(df_twits.groupby(['base_asset', df_twits['date'].dt.floor('h')]).first().unstack('base_asset')['id'].isna(), cmap=sns.cm.rocket_r, cbar=False)\n",
    "ax.set_yticklabels([ ylabel.get_text().split('T')[0] for ylabel in ax.get_yticklabels() ])\n",
    "ax.set_xlabel(\"Crypto Asset\")\n",
    "ax.set_ylabel(\"Date\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.savefig(f\"{saving_folder}/imgs/twit_missing_data.{saving_format}\", format=saving_format, dpi=dpi, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Number of Twits per Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tmp = df_twits.groupby('label', dropna=False).size().to_frame('Number of Samples').rename_axis(None).pivot_table(\n",
    "    index=pd.Index(['Bearish', 'Bullish', 'NaN']),\n",
    "    margins=True,\n",
    "    margins_name='TOTAL',\n",
    "    aggfunc=np.sum\n",
    ").style.format('{:,}')\n",
    "\n",
    "df_tmp.to_latex(\n",
    "    f\"{saving_folder}/tables/twit_count_per_label.tex\",\n",
    "    caption=\"Number of twits collected per label\",\n",
    "    label=\"table:twit_count_per_label\",\n",
    "    position_float='centering',\n",
    "    position='H',\n",
    "    hrules=True\n",
    ")\n",
    "df_tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Twit Count Over Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = df_twits[['date', 'label']].groupby([df_twits['date'].dt.to_period('M').dt.to_timestamp(), 'label'], dropna=False).size().unstack('label').plot(kind='bar', stacked=True, rot=45)\n",
    "ax.set_xticklabels([ datetime.fromisoformat(xlabel.get_text()).strftime(\"%b %Y\") for xlabel in ax.get_xticklabels() ])\n",
    "plt.ylabel(\"Number of Twits\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.legend(title=\"Label\")\n",
    "plt.savefig(f\"{saving_folder}/imgs/twit_count_over_time.{saving_format}\", format=saving_format, dpi=dpi, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Twit Count per Crypto Asset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_twits[['base_asset', 'label']].groupby(['base_asset', 'label'], dropna=False).size().unstack('label').plot(kind='bar', stacked=True, rot=45)\n",
    "plt.ylabel(\"Number of Twits\")\n",
    "plt.xlabel(\"Crypto Asset\")\n",
    "plt.legend(title=\"Label\")\n",
    "plt.savefig(f\"{saving_folder}/imgs/twit_count_per_crypto_asset.{saving_format}\", format=saving_format, dpi=dpi, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(24, 24))\n",
    "\n",
    "# Sets the axis\n",
    "ax0 = plt.subplot2grid((2, 2), (0, 0), colspan=1, fig=fig)\n",
    "ax1 = plt.subplot2grid((2, 2), (0, 1), colspan=1, fig=fig)\n",
    "ax2 = plt.subplot2grid((2, 2), (1, 0), colspan=2, fig=fig)\n",
    "\n",
    "df_twits[['label', 'user.type']].groupby(['label', 'user.type'], dropna=False).size().unstack('user.type').plot(kind='bar', stacked=True, rot=0, ax=ax0)\n",
    "ax0.set_ylabel(\"Number of Twits\")\n",
    "ax0.set_xlabel(\"Label\")\n",
    "ax0.legend(title=\"User Type\")\n",
    "\n",
    "df_twits[['date', 'user.type']].groupby([df_twits['date'].dt.to_period('M').dt.to_timestamp(), 'user.type'], dropna=False).size().unstack('user.type').plot(kind='bar', stacked=True, rot=90, ax=ax1)\n",
    "ax1.set_xticklabels([ datetime.fromisoformat(xlabel.get_text()).strftime(\"%b %Y\") for xlabel in ax1.get_xticklabels() ])\n",
    "ax1.set_ylabel(\"Number of Twits\")\n",
    "ax1.set_xlabel(\"Date\")\n",
    "ax1.legend(title=\"User Type\")\n",
    "\n",
    "df_twits[['base_asset', 'user.type']].groupby(['base_asset', 'user.type'], dropna=False).size().unstack('user.type').plot(kind='bar', stacked=True, rot=45, ax=ax2)\n",
    "ax2.set_ylabel(\"Number of Twits\")\n",
    "ax2.set_xlabel(\"Crypto Asset\")\n",
    "ax2.legend(title=\"User Type\")\n",
    "\n",
    "plt.savefig(f\"{saving_folder}/imgs/bot_human_comparison.{saving_format}\", format=saving_format, dpi=dpi, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Average Engagement Rate per User Type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twits Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtains a list of words that shouldn't be considered\n",
    "stopwords = df_cryptomap['base_asset'].str.lower().tolist() + df_cryptomap['name'].str.lower().tolist()\n",
    "stopwords += [ \n",
    "    \"...\", \"will\", \"going\", \"see\", \"let\", \"one\", \"next\", \"still\", \"know\", \"time\", \"back\", \"coin\", \n",
    "    \"price\", \"new\", \"day\", \"don\", \"think\", \"today\", \"soon\", \"last\", \"night\" \n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bearish"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Human"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = (df_twits['label'] == 'Bearish') & (df_twits['user.type'] == 'Human')\n",
    "txt = df_twits[mask]['text_heavy_clean'].str.cat(sep=' ').encode('ascii', 'ignore').decode('ascii')\n",
    "txt = re.compile(r'\\b%s\\b' % r'\\b|\\b'.join(map(re.escape, stopwords))).sub('', txt)\n",
    "txt = re.sub(r\"\\b\\w{1,2}\\b\", \"\", txt)\n",
    "txt = txt.translate(str.maketrans('', '', string.punctuation))\n",
    "wfreq = collections.Counter(txt.split()).most_common(30)\n",
    "words, freqs = zip(*wfreq[1:])\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.bar(words, freqs)\n",
    "ax = plt.gca()\n",
    "ax.set_xticklabels(map(lambda x: textwrap.fill(x, 10), words), rotation=90)\n",
    "plt.savefig(f\"{saving_folder}/imgs/human_bearish_word_freq.{saving_format}\", format=saving_format, dpi=dpi, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = (df_twits['label'] == 'Bearish') & (df_twits['user.type'] == 'Bot')\n",
    "txt = df_twits[mask]['text_heavy_clean'].str.cat(sep=' ').encode('ascii', 'ignore').decode('ascii')\n",
    "txt = re.compile(r'\\b%s\\b' % r'\\b|\\b'.join(map(re.escape, stopwords))).sub('', txt)\n",
    "txt = re.sub(r\"\\b\\w{1,2}\\b\", \"\", txt)\n",
    "txt = txt.translate(str.maketrans('', '', string.punctuation))\n",
    "wfreq = collections.Counter(txt.split()).most_common(30)\n",
    "words, freqs = zip(*wfreq[1:])\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.bar(words, freqs)\n",
    "ax = plt.gca()\n",
    "ax.set_xticklabels(map(lambda x: textwrap.fill(x, 10), words), rotation=90)\n",
    "plt.savefig(f\"{saving_folder}/imgs/bot_bearish_word_freq.{saving_format}\", format=saving_format, dpi=dpi, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bullish"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Humans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = (df_twits['label'] == 'Bullish') & (df_twits['user.type'] == 'Human')\n",
    "txt = df_twits[mask]['text_heavy_clean'].str.cat(sep=' ').encode('ascii', 'ignore').decode('ascii')\n",
    "txt = re.compile(r'\\b%s\\b' % r'\\b|\\b'.join(map(re.escape, stopwords))).sub('', txt)\n",
    "txt = re.sub(r\"\\b\\w{1,2}\\b\", \"\", txt)\n",
    "txt = txt.translate(str.maketrans('', '', string.punctuation))\n",
    "wfreq = collections.Counter(txt.split()).most_common(30)\n",
    "words, freqs = zip(*wfreq[1:])\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.bar(words, freqs)\n",
    "ax = plt.gca()\n",
    "ax.set_xticklabels(map(lambda x: textwrap.fill(x, 10), words), rotation=90)\n",
    "plt.savefig(f\"{saving_folder}/imgs/human_bullish_word_freq.{saving_format}\", format=saving_format, dpi=dpi, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = (df_twits['label'] == 'Bullish') & (df_twits['user.type'] == 'Bot')\n",
    "txt = df_twits[mask]['text_heavy_clean'].str.cat(sep=' ').encode('ascii', 'ignore').decode('ascii')\n",
    "txt = re.compile(r'\\b%s\\b' % r'\\b|\\b'.join(map(re.escape, stopwords))).sub('', txt)\n",
    "txt = re.sub(r\"\\b\\w{1,2}\\b\", \"\", txt)\n",
    "txt = txt.translate(str.maketrans('', '', string.punctuation))\n",
    "wfreq = collections.Counter(txt.split()).most_common(30)\n",
    "words, freqs = zip(*wfreq)\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.bar(words, freqs)\n",
    "ax = plt.gca()\n",
    "ax.set_xticklabels(map(lambda x: textwrap.fill(x, 10), words), rotation=90)\n",
    "plt.savefig(f\"{saving_folder}/imgs/bot_bullish_word_freq.{saving_format}\", format=saving_format, dpi=dpi, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tmp = df_twits[['user.type', 'text_heavy_clean']]\n",
    "df_tmp['wcount'] = df_tmp['text_heavy_clean'].str.split().str.len()\n",
    "\n",
    "_ = plt.figure(figsize=(15, 7))\n",
    "ax = plt.gca()\n",
    "\n",
    "sns.histplot(df_tmp, x='wcount', bins=100, log_scale=(False, True), hue='user.type', ax=ax)\n",
    "ax.set_xlabel(\"Twit Word Count\")\n",
    "plt.ylabel(\"Number of Twits\")\n",
    "plt.legend(['Human', 'Bot'], title=\"User Type\")\n",
    "plt.savefig(f\"{saving_folder}/imgs/wcount_distribution.{saving_format}\", format=saving_format, dpi=dpi, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tmp = df_twits[df_twits['id'] == 355849225][['text', 'text_light_clean', 'text_heavy_clean']]\n",
    "df_tmp['text'] = df_tmp['text'].str.replace(\" ass \", \" \")\n",
    "df_tmp['text_light_clean'] = df_tmp['text_light_clean'].str.replace(\" ass \", \" \")\n",
    "df_tmp['text_heavy_clean'] = df_tmp['text_heavy_clean'].str.replace(\" ass \", \" \")\n",
    "df_tmp['text_heavy_clean'] = df_tmp['text_heavy_clean'].str.replace(\" fire fire fire \", \" fire \")\n",
    "df_tmp.rename(columns={'text': \"Original Text\", 'text_light_clean': \"Light Text Cleaning\", 'text_heavy_clean': \"Heavy Text Cleaning\"}, inplace=True)\n",
    "df_tmp = df_tmp.T\n",
    "df_tmp.rename(columns={290542: \"Twit Corpus\"}, inplace=True)\n",
    "df_tmp.style.to_latex(\n",
    "    f\"{saving_folder}/tables/eg_text_cleaning.tex\",\n",
    "    caption=\"Example of light and heavy text cleaning to a twit corpus\",\n",
    "    label=\"table:eg_text_cleaning\",\n",
    "    position_float='centering',\n",
    "    position='H',\n",
    "    hrules=True\n",
    ")\n",
    "df_tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# history = pd.read_csv(\"./models/bert/history.csv\", index_col=0)\n",
    "\n",
    "df_test = df_twits.drop_duplicates(subset=['id']).dropna(subset=['label']).sample(3086782)\n",
    "y_test = df_test['label'].replace({ 'Bearish': 0, 'Bullish': 1 })\n",
    "y_test_scores = df_test['label_pred_score']\n",
    "y_test_pred = df_test['label_pred'].replace({ 'Bearish': 0, 'Bullish': 1 })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, axs = plt.subplots(1, 3, figsize=(30, 12))\n",
    "\n",
    "axs[0].plot(history['loss'], label='Train')\n",
    "axs[0].plot(history['val_loss'], label='Validation')\n",
    "axs[0].set_title(\"Loss Value\")\n",
    "axs[0].set_ylabel(\"Loss Value\")\n",
    "axs[0].set_xlabel(\"Epoch\")\n",
    "axs[0].legend()\n",
    "\n",
    "axs[1].plot(history['accuracy'], label='Train')\n",
    "axs[1].plot(history['val_accuracy'], label='Validation')\n",
    "axs[1].set_title(\"Accuracy\")\n",
    "axs[1].set_ylabel(\"Accuracy\")\n",
    "axs[1].set_xlabel(\"Epoch\")\n",
    "axs[1].legend()\n",
    "\n",
    "axs[2].plot(history['auc'], label='Train')\n",
    "axs[2].plot(history['val_auc'], label='Validation')\n",
    "axs[2].set_title(\"AUC\")\n",
    "axs[2].set_ylabel(\"AUC\")\n",
    "axs[2].set_xlabel(\"Epoch\")\n",
    "axs[2].legend()\n",
    "\n",
    "plt.savefig(f\"{saving_folder}/imgs/bert_training_metrics.{saving_format}\", format=saving_format, dpi=dpi, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtains the confusion matrix\n",
    "cf_matrix = confusion_matrix(y_test, y_test_pred)\n",
    "\n",
    "# Obtains the annotations\n",
    "counts = [ f\"{val:0.0f}\" for val in cf_matrix.flatten() ]\n",
    "pcts = [ f\"{100*val:.2f}\" for val in cf_matrix.flatten()/np.sum(cf_matrix) ]\n",
    "annot = np.asarray([ f\"{count}\\n({pct}%)\" for count, pct in zip(counts, pcts) ]).reshape(2, 2)\n",
    "\n",
    "# Plots the confusion matrix\n",
    "ax = sns.heatmap(cf_matrix, annot=annot, cmap='Blues', fmt='')\n",
    "ax.set_ylabel(\"True\", fontsize=20)\n",
    "ax.set_xlabel(\"Predicted\", fontsize=20)\n",
    "ax.xaxis.set_ticklabels(['Bearish', 'Bullish'], fontsize=10) \n",
    "ax.yaxis.set_ticklabels(['Bearish', 'Bullish'], fontsize=10)\n",
    "plt.savefig(f\"{saving_folder}/imgs/bert_confusion_matrix.{saving_format}\", format=saving_format, dpi=dpi, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROC Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get true positive rates and false positive rates\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_test_scores)\n",
    "\n",
    "# Plots the ROC curve\n",
    "plt.plot(fpr, tpr)\n",
    "plt.plot([0, 1], [0, 1], linestyle=\"--\")\n",
    "\n",
    "# Shades the AUC and show its value\n",
    "filled_part = plt.fill_between(fpr, tpr, color='#8EB9D7')\n",
    "(x0, y0), (x1, y1) = filled_part.get_paths()[0].get_extents().get_points()\n",
    "plt.text(x1/2, y1/3, f\"AUC = {roc_auc_score(y_test, y_test_scores):.3f}\", fontsize=16)\n",
    "\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.savefig(f\"{saving_folder}/imgs/bert_roc.{saving_format}\", format=saving_format, dpi=dpi, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtains the classification report as a dataframe\n",
    "df_clf_report = pd.DataFrame(classification_report(y_test, y_test_pred, output_dict=True)).rename(columns={'0': 'Bearish', '1': 'Bullish'}).transpose()\n",
    "df_clf_report.style.format(formatter='{:,.3f}').to_latex(\n",
    "    f\"{saving_folder}/tables/bert_classification_report.tex\",\n",
    "    caption=\"Classification report of the BERT based neural network\",\n",
    "    label=\"table:bert_classification_report\",\n",
    "    position_float='centering',\n",
    "    position='H',\n",
    "    hrules=True\n",
    ")\n",
    "df_clf_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selects only useful columns\n",
    "df_tmp = pd.read_csv(\"./datasets/enhanced/twits.csv.gz\", low_memory=False)[['id', 'user.type', 'base_asset', 'text', 'label']].dropna()\n",
    "\n",
    "# Drop duplicates on id (same twit can tag multiple base_asset)\n",
    "df_tmp.drop_duplicates('id', ignore_index=True, inplace=True)\n",
    "\n",
    "# Gets a small sample of the dataset for training and testing (balanced labels and base_assets)\n",
    "df_train = df_tmp.groupby('base_asset', group_keys=False).apply(lambda x: x.groupby('label', group_keys=False).apply(lambda y: y.sample(x['label'].value_counts().min())))\n",
    "# df_val = df_tmp[~df_tmp['id'].isin(df_train['id'])].groupby('base_asset', group_keys=False).apply(lambda x: x.groupby('label', group_keys=False).apply(lambda y: y.sample(x['label'].value_counts().min())))\n",
    "# df_test = df_tmp[(~df_tmp['id'].isin(df_train['id'])) & (~df_tmp['id'].isin(df_val['id']))]\n",
    "\n",
    "# df_tmp.loc[df_tmp['id'].isin(df_train['id']), 'set'] = 'Train'\n",
    "# df_tmp.loc[df_tmp['id'].isin(df_val['id']), 'set'] = 'Validation'\n",
    "# df_tmp.loc[df_tmp['id'].isin(df_test['id']), 'set'] = 'Test'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_metric = pd.read_csv(\"./datasets/engagement_rate.csv.gz\", index_col=0, header=[0, 1])\n",
    "base_assets = ['BTC', 'ETH', 'DOGE', 'SHIB']\n",
    "cols = ['Bearish Human', 'Bullish Human', 'Bearish Bot', 'Bullish Bot']\n",
    "col_map = {'er_bear_human': 'Bearish Human', 'er_bull_human': 'Bullish Human', 'er_bear_bot': 'Bearish Bot', 'er_bull_bot': 'Bullish Bot'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_btc = df_metric['BTC'].rename(columns=col_map)\n",
    "df_btc.index = pd.to_datetime(df_btc.index)\n",
    "\n",
    "axs = df_btc[cols].plot(subplots=True, layout=(2, 2), sharex=False, legend=False, figsize=(30, 25))\n",
    "axs[0, 0].set_title(\"Bearish Twits Made by Bots\")\n",
    "axs[0, 1].set_title(\"Bearish Twits Made by Humans\")\n",
    "axs[1, 0].set_title(\"Bullish Twits Made by Bots\")\n",
    "axs[1, 1].set_title(\"Bullish Twits Made by Humans\")\n",
    "for ax in axs.flatten():\n",
    "    ax.set_xlabel(\"Date\")\n",
    "    ax.set_ylabel(\"Average Engagement Rate\")\n",
    "\n",
    "plt.savefig(f\"{saving_folder}/imgs/sentiment_metric_btc.{saving_format}\", format=saving_format, dpi=dpi, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_btc.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BTC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_btc = df_metric['BTC'].rename(columns=col_map)\n",
    "df_btc.index = pd.to_datetime(df_btc.index)\n",
    "df_btc['Return'] = df_btc['price'].pct_change()\n",
    "df_btc = df_btc[cols+['Return']].dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix = df_btc.corr()\n",
    "mask = np.triu(corr_matrix)\n",
    "\n",
    "sns.heatmap(corr_matrix, annot=True, mask=mask, cmap='Blues')\n",
    "plt.xticks(rotation=45)\n",
    "plt.savefig(f\"{saving_folder}/imgs/btc_corr_matrix.{saving_format}\", format=saving_format, dpi=dpi, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Causality Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check Stationarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_btc_stationay = df_btc.copy()\n",
    "summary = { col: 0 for col in cols+['Return'] }\n",
    "for col in cols:\n",
    "    t_statistic, pvalue, usedlag, nobs, critical_values, icbest = adfuller(df_btc[col])\n",
    "\n",
    "    while pvalue > 0.05:\n",
    "        df_btc_stationay[col] = df_btc_stationay[col].diff() \n",
    "        t_statistic, pvalue, usedlag, nobs, critical_values, icbest = adfuller(df_btc[col])\n",
    "        summary[col] += 1\n",
    "\n",
    "summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Obtains the Number of Lags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VAR(df_btc)\n",
    "\n",
    "best_model_fit = None\n",
    "for maxlag in range(1, 50):\n",
    "    model_fit = model.fit(maxlag)\n",
    "    if best_model_fit == None:\n",
    "        best_model_fit = model_fit\n",
    "    else:\n",
    "        if model_fit.aic < best_model_fit.aic:\n",
    "            best_model_fit = model_fit\n",
    "\n",
    "maxlag = best_model_fit.k_ar\n",
    "maxlag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Obtains the Granger Causality Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tmp = grangers_causation_matrix(df_btc_stationay, maxlag=maxlag)\n",
    "\n",
    "sns.heatmap(df_tmp, annot=True, cmap='Blues')\n",
    "plt.xticks(rotation=45)\n",
    "plt.savefig(f\"{saving_folder}/imgs/causality_btc.{saving_format}\", format=saving_format, dpi=dpi, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "df_btc[cols] = scaler.fit_transform(df_btc[cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtains the endogenous and exogenous variables\n",
    "y = df_btc['Return']\n",
    "X = sm.add_constant(df_btc[cols])\n",
    "\n",
    "# Runs the OLS regression\n",
    "model = sm.OLS(y, X)\n",
    "model_fit = model.fit()\n",
    "with open(f\"{saving_folder}/tables/ols_btc.tex\", 'w+') as f:\n",
    "    f.write(model_fit.summary().as_latex())\n",
    "print(model_fit.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ETH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eth = df_metric['ETH'].rename(columns=col_map)\n",
    "df_eth.index = pd.to_datetime(df_eth.index)\n",
    "df_eth['Return'] = df_eth['price'].pct_change()\n",
    "df_eth = df_eth[cols+['Return']].dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix = df_eth.corr()\n",
    "mask = np.triu(corr_matrix)\n",
    "\n",
    "sns.heatmap(corr_matrix, annot=True, mask=mask, cmap='Blues')\n",
    "plt.xticks(rotation=45)\n",
    "plt.savefig(f\"{saving_folder}/imgs/eth_corr_matrix.{saving_format}\", format=saving_format, dpi=dpi, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Causality Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check Stationarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eth_stationay = df_eth.copy()\n",
    "summary = { col: 0 for col in cols+['Return'] }\n",
    "for col in cols:\n",
    "    t_statistic, pvalue, usedlag, nobs, critical_values, icbest = adfuller(df_eth[col])\n",
    "\n",
    "    while pvalue > 0.05:\n",
    "        df_eth_stationay[col] = df_eth_stationay[col].diff() \n",
    "        t_statistic, pvalue, usedlag, nobs, critical_values, icbest = adfuller(df_eth[col])\n",
    "        summary[col] += 1\n",
    "\n",
    "summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Obtains the Number of Lags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VAR(df_eth)\n",
    "\n",
    "best_model_fit = None\n",
    "for maxlag in range(1, 50):\n",
    "    model_fit = model.fit(maxlag)\n",
    "    if best_model_fit == None:\n",
    "        best_model_fit = model_fit\n",
    "    else:\n",
    "        if model_fit.aic < best_model_fit.aic:\n",
    "            best_model_fit = model_fit\n",
    "\n",
    "maxlag = best_model_fit.k_ar\n",
    "maxlag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Obtains the Granger Causality Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tmp = grangers_causation_matrix(df_eth_stationay, maxlag=maxlag)\n",
    "\n",
    "sns.heatmap(df_tmp, annot=True, cmap='Blues')\n",
    "plt.xticks(rotation=45)\n",
    "plt.savefig(f\"{saving_folder}/imgs/causality_eth.{saving_format}\", format=saving_format, dpi=dpi, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "df_eth[cols] = scaler.fit_transform(df_eth[cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtains the endogenous and exogenous variables\n",
    "y = df_eth['Return']\n",
    "X = sm.add_constant(df_eth[cols])\n",
    "\n",
    "# Runs the OLS regression\n",
    "model = sm.OLS(y, X)\n",
    "model_fit = model.fit()\n",
    "with open(f\"{saving_folder}/tables/ols_eth.tex\", 'w+') as f:\n",
    "    f.write(model_fit.summary().as_latex())\n",
    "print(model_fit.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DOGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_doge = df_metric['DOGE'].rename(columns=col_map)\n",
    "df_doge.index = pd.to_datetime(df_doge.index)\n",
    "df_doge['Return'] = df_doge['price'].pct_change()\n",
    "df_doge = df_doge[cols+['Return']].dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix = df_doge.corr()\n",
    "mask = np.triu(corr_matrix)\n",
    "\n",
    "sns.heatmap(corr_matrix, annot=True, mask=mask, cmap='Blues')\n",
    "plt.xticks(rotation=45)\n",
    "plt.savefig(f\"{saving_folder}/imgs/doge_corr_matrix.{saving_format}\", format=saving_format, dpi=dpi, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Causality Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check Stationarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_doge_stationay = df_doge.copy()\n",
    "summary = { col: 0 for col in cols+['Return'] }\n",
    "for col in cols:\n",
    "    t_statistic, pvalue, usedlag, nobs, critical_values, icbest = adfuller(df_doge[col])\n",
    "\n",
    "    while pvalue > 0.05:\n",
    "        df_doge_stationay[col] = df_doge_stationay[col].diff() \n",
    "        t_statistic, pvalue, usedlag, nobs, critical_values, icbest = adfuller(df_doge[col])\n",
    "        summary[col] += 1\n",
    "\n",
    "summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Obtains the Number of Lags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VAR(df_doge)\n",
    "\n",
    "best_model_fit = None\n",
    "for maxlag in range(1, 50):\n",
    "    model_fit = model.fit(maxlag)\n",
    "    if best_model_fit == None:\n",
    "        best_model_fit = model_fit\n",
    "    else:\n",
    "        if model_fit.aic < best_model_fit.aic:\n",
    "            best_model_fit = model_fit\n",
    "\n",
    "maxlag = best_model_fit.k_ar\n",
    "maxlag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Obtains the Granger Causality Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tmp = grangers_causation_matrix(df_doge_stationay, maxlag=maxlag)\n",
    "\n",
    "sns.heatmap(df_tmp, annot=True, cmap='Blues')\n",
    "plt.xticks(rotation=45)\n",
    "plt.savefig(f\"{saving_folder}/imgs/causality_doge.{saving_format}\", format=saving_format, dpi=dpi, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "df_doge[cols] = scaler.fit_transform(df_doge[cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtains the endogenous and exogenous variables\n",
    "y = df_doge['Return']\n",
    "X = sm.add_constant(df_doge[cols])\n",
    "\n",
    "# Runs the OLS regression\n",
    "model = sm.OLS(y, X)\n",
    "model_fit = model.fit()\n",
    "with open(f\"{saving_folder}/tables/ols_doge.tex\", 'w+') as f:\n",
    "    f.write(model_fit.summary().as_latex())\n",
    "print(model_fit.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
