{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Master Thesis: Sentiment Classification Models\n",
    "\n",
    "#### The Influence of Bots in the Crypto Market\n",
    "\n",
    "*By Daniel Jorge Deutsch*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import re\n",
    "import ssl\n",
    "import string\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text as text\n",
    "from IPython.display import clear_output\n",
    "from sklearn.metrics import (classification_report, confusion_matrix,\n",
    "                             roc_auc_score, roc_curve)\n",
    "from sklearn.model_selection import train_test_split\n",
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ignore warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Creates a default https context\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "# Matplotlib styles\n",
    "plt.style.use('ggplot')\n",
    "plt.rcParams.update({\n",
    "    'figure.figsize': (15, 6),\n",
    "    'axes.prop_cycle': plt.cycler(color=['#4C72B0', '#C44E52', '#55A868', '#8172B2', '#CCB974', '#64B5CD']),\n",
    "    'axes.facecolor': '#EAEAF2'\n",
    "})\n",
    "\n",
    "# Constants\n",
    "START_DATE = datetime(2019, 6, 1)\n",
    "END_DATE = datetime(2022, 6, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auxiliary Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def render_df_table(data, col_width=3.0, row_height=0.625, font_size=14, header_color='#40466e', row_colors=['#f1f1f2', 'w'], edge_color='w', bbox=[0, 0, 1, 1], header_columns=0, ax=None, **kwargs):\n",
    "    if ax is None:\n",
    "        size = (np.array(data.shape[::-1]) + np.array([0, 1])) * np.array([col_width, row_height])\n",
    "        _, ax = plt.subplots(figsize=size)\n",
    "        ax.axis('off')\n",
    "    mpl_table = ax.table(cellText=data.round(3).values, bbox=bbox, colLabels=data.columns, **kwargs)\n",
    "    mpl_table.auto_set_font_size(False)\n",
    "    mpl_table.set_fontsize(font_size)\n",
    "\n",
    "    for k, cell in mpl_table._cells.items():\n",
    "        cell.set_edgecolor(edge_color)\n",
    "        if k[0] == 0 or k[1] < header_columns:\n",
    "            cell.set_text_props(weight='bold', color='w')\n",
    "            cell.set_facecolor(header_color)\n",
    "        else:\n",
    "            cell.set_facecolor(row_colors[k[0]%len(row_colors) ])\n",
    "    return ax.get_figure(), ax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Import"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Twits Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we load the pandas dataframe with the processed dataset containing more than 7 million twits (posted between 2019-06-01 and 2022-06-01) about the top 50 crypto assets listed in the Binance exchange. The dataset has the following structure:\n",
    "\n",
    "| Index &nbsp; | Name                      | Description                                                   |\n",
    "|--------------|:--------------------------|--------------------------------------------------------------:|\n",
    "|  0           | `id`                      | Id of the twit post                                           |\n",
    "|  1           | `date`                    | Date of twit creation                                         |\n",
    "|  2           | `base_asset`              | Crypto asset related to the twit                              |\n",
    "|  3           | `user_id`                 | Id of the user who posted the twit                            |\n",
    "|  4           | `text`                    | Twit raw corpus                                               |\n",
    "|  5           | `text_light_clean`        | Twit corpus with light text cleaning                          |\n",
    "|  6           | `text_heavy_clean`        | Twit corpus with heavy text cleaning                          |\n",
    "|  7           | `n_likes`                 | Number of twit post likes                                     |\n",
    "|  8           | `n_reshares`              | Number of twit post reshares dependents                       |\n",
    "|  9           | `label`                   | Whether the twit has a bearish or a bullish sentiment         |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./datasets/enhanced/twits.csv.gz\", index_col=0, parse_dates=['date'], low_memory=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Users Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the twits dataframe, we are able to derive a dataset with unique user information. This dataset contains over 160 thousand users and has the following features:\n",
    "\n",
    "| Index &nbsp; | Name &nbsp; &nbsp; &nbsp; | Description                                                   |\n",
    "|--------------|:--------------------------|--------------------------------------------------------------:|\n",
    "|  0           | `user_id`                 | Id of the user who posted the twit                            |\n",
    "|  1           | `n_user_followers`        | Number of users that follow the user that posted the twit     |\n",
    "|  2           | `n_user_following`        | Number of users that the user that posted the twit follows    |\n",
    "|  3           | `user_join_date`          | Date when the user created his account                        |\n",
    "|  4           | `n_user_ideas`            | Total number of twits made by the user since his joining date |\n",
    "|  5           | `is_user_official`        | =True if the user is official                                 |\n",
    "|  6           | `n_user_watchlist_stocks` | Number of stocks in the user's watchlist                      |\n",
    "|  7           | `n_user_likes`            | Total number of twits that the user have liked                |\n",
    "|  8           | `n_user_twits`            | Number of twits of the user that are in the twits dataset     |\n",
    "|  9           | `n_active_days`           | Number of days since user's joining date                      |\n",
    "| 10           | `n_active_days_clipped`   | Number of days since max(START_DATE, user's joining date)     |\n",
    "| 11           | `user_twit_freq`          | Daily frequency of user twits                                 |\n",
    "| 12           | `user_idea_freq`          | Daily frequency of user ideas                                 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_users = pd.read_csv(\"./datasets/enhanced/users.csv.gz\", index_col=0, parse_dates=['join_date'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we deepen into the sentiment analysis models, it is important to properly understand the data that we want to classify. A good way to do it is by plotting different visualizations of it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ideas per User Distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An idea in StockTwits is how they name a post (or what I call a twit). Here we can see that most of the users tend to post a a couple thousands of twits, but there are outliers that have posted over 200,000 twits.\n",
    "\n",
    "Considering that stocktwits was launched in 2009, since then we would have 2022-2009=13 years of usage i.e., 13*365=4745 days. This means that the user who posted 200,000 twits (considering he was created in 2009), would have to post 200000/4745=42 twits a day to reach this mark. According to a study made by ___, the average user on Twitter (a platform with a lot more frequent users), tend to post on average 2.6 twits a day. By looking at this two informations, it is clear that there are lots of bot accounts in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(df_users['n_user_ideas'], bins=300, log_scale=(False, True), kde=False)\n",
    "plt.title(\"Log-Scaled Distribution of the Number of Ideas Posted by each User\")\n",
    "plt.xlabel(\"Number of Ideas Posted by the User\")\n",
    "plt.savefig(\"./imgs/idea_count_per_user_distribution.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ideas Frequency Per User"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following plot it is easier to see what was explained above. There are several users with posting frequency way above expected from a human."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(df_users['user_idea_freq'], bins=300, log_scale=(False, True), kde=False)\n",
    "plt.title(\"Log-Scaled Distribution of the Frequency of Ideas Posted by each User\")\n",
    "plt.xlabel(\"Frequency of Ideas Posted by the User\")\n",
    "plt.savefig(\"./imgs/idea_freq_per_user_distribution.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Twit Count per User Distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we focus on the dataset itself, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(df_users['n_user_twits'], bins=300, log_scale=(False, True), kde=False)\n",
    "plt.title(\"Log-Scaled Distribution of the Number of Twits Posted by each User\")\n",
    "plt.xlabel(\"Number of Twits Posted by the User\")\n",
    "plt.savefig(\"./imgs/twit_count_per_user_distribution.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Daily Twit Frequency Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(df_users['user_twit_freq'], bins=300, log_scale=(False, True), kde=False)\n",
    "plt.title(\"Log-Scaled Distribution of the Daily Posting Frequency of each User\")\n",
    "plt.xlabel(\"Number of Twits Posted by the User per day\")\n",
    "plt.savefig(\"./imgs/twit_daily_freq_per_user_distribution.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User Followers Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(df_users['n_user_following'], bins=300, log_scale=(False, True), kde=False)\n",
    "plt.title(\"Log-Scaled Distribution of the Frequency of Ideas Posted by each User\")\n",
    "plt.xlabel(\"Frequency of Ideas Posted by the User\")\n",
    "plt.savefig(\"./imgs/idea_freq_per_user_distribution.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Twits Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General Data Insights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Label Count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this plot we can clearly observe that we are considering a very unbalanced dataset. Almost 4 million twits are marked as having a bullish sentiment behind it and no more than 500 thousand were marked as having a bearish sentiment. We can also observe that a vary considerate amount of twits (over 2 million of them) don't have a label showing the sentiment behind it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['label', 'user_type']].groupby(['label', 'user_type'], dropna=False).size().unstack('user_type').plot(kind='bar', stacked=True, rot=45)\n",
    "plt.title(\"Twit Count per Label\")\n",
    "plt.ylabel(\"Number of Twits\")\n",
    "plt.xlabel(\"Label\")\n",
    "plt.savefig(\"./imgs/twit_count_per_label.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Twit Count Over Time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This plot shows us the evolution of twit volume over time. We can observe that there was a huge increase in twits about crypto assets in the year 2021 that held until 2022. It is worth noticing that basically during the time considered, there is almos always a higher volume of bullish twits over bearish ones. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['date', 'label']].groupby([df['date'].dt.date, 'label'], dropna=False).size().unstack('label').plot(lw=0.7, rot=45)\n",
    "plt.title(\"Twit Count Over Time\")\n",
    "plt.ylabel(\"Number of Twits\")\n",
    "plt.xlabel(\"Time\")\n",
    "plt.savefig(\"./imgs/twit_count_over_time.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Twit Count per Month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['date', 'label']].groupby([df['date'].dt.month_name(), 'label'], dropna=False).size().unstack('label').reindex(['January', 'February', 'March', 'April', 'May', 'June', 'July', 'August', 'September', 'October', 'November', 'December']).plot(kind='bar', stacked=True, rot=45)\n",
    "plt.title(\"Twit Count per Month\")\n",
    "plt.ylabel(\"Number of Twits\")\n",
    "plt.xlabel(\"Month\")\n",
    "plt.savefig(\"./imgs/twit_count_per_month.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Twit Count per Year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['date', 'label']].groupby([df['date'].dt.year, 'label'], dropna=False).size().unstack('label').plot(kind='bar', stacked=True, rot=45)\n",
    "plt.title(\"Twit Count per Year\")\n",
    "plt.ylabel(\"Number of Twits\")\n",
    "plt.xlabel(\"Year\")\n",
    "plt.savefig(\"./imgs/twit_count_per_year.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Twit Count per Crypto Asset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the following plot, it is clear that there are a few crypto assets with a bigger representation in the dataset when compared to othe ones. We can se that most of the crypto with higher twit volume are \"hyped crypto\" that receive more attention from the media such as BTC, ETH, DOGE and SHIB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['base_asset', 'label']].groupby(['base_asset', 'label'], dropna=False).size().unstack('label').plot(kind='bar', stacked=True, rot=45)\n",
    "plt.title(\"Twit Count per Crypto Asset\")\n",
    "plt.ylabel(\"Number of Twits\")\n",
    "plt.xlabel(\"Crypto Asset\")\n",
    "plt.savefig(\"./imgs/twit_count_per_crypto_asset.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word Clouds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Considering now the corpus of each twit, we can se the most used words in each category. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(21, 10))\n",
    "\n",
    "# Sets the axis\n",
    "ax0 = plt.subplot2grid((2, 3), (0, 0), colspan=3, fig=fig)\n",
    "ax1 = plt.subplot2grid((2, 3), (1, 0), colspan=1, fig=fig)\n",
    "ax2 = plt.subplot2grid((2, 3), (1, 1), colspan=1, fig=fig)\n",
    "ax3 = plt.subplot2grid((2, 3), (1, 2), colspan=1, fig=fig)\n",
    "\n",
    "# All twits plot\n",
    "txt_all = df['text_heavy_clean'].str.cat(sep=' ')\n",
    "wc0 = WordCloud(width=735, height=175, collocations=False, background_color='white').generate(txt_all)\n",
    "ax0.set_title(\"All Twits WordCloud\", fontsize=20)\n",
    "ax0.set_axis_off()\n",
    "ax0.imshow(wc0)\n",
    "\n",
    "# Bearish twits plot\n",
    "txt_bearish = df[df['label'] == 'Bearish']['text_heavy_clean'].str.cat(sep=' ')\n",
    "wc1 = WordCloud(width=735, height=525, collocations=False, background_color='white').generate(txt_bearish)\n",
    "ax1.set_title(\"Bearish Twits WordCloud\", fontsize=20)\n",
    "ax1.set_axis_off()\n",
    "ax1.imshow(wc1)\n",
    "\n",
    "# Bullish twits plot\n",
    "txt_bullish = df[df['label'] == 'Bullish']['text_heavy_clean'].str.cat(sep=' ')\n",
    "wc2 = WordCloud(width=735, height=525, collocations=False, background_color='white').generate(txt_bullish)\n",
    "ax2.set_title(\"Bullish Twits WordCloud\", fontsize=20)\n",
    "ax2.set_axis_off()\n",
    "ax2.imshow(wc2)\n",
    "\n",
    "# Non-labeld twits plot\n",
    "txt_nan = df[df['label'].isna()]['text_heavy_clean'].str.cat(sep=' ')\n",
    "wc3 = WordCloud(width=735, height=525, collocations=False, background_color='white').generate(txt_nan)\n",
    "ax3.set_title(\"Non-Labeled Twits WordCloud\", fontsize=20)\n",
    "ax3.set_axis_off()\n",
    "ax3.imshow(wc3)\n",
    "\n",
    "plt.savefig(\"./imgs/wordclouds.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Top Most Frequent Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(21, 15))\n",
    "\n",
    "# Sets the axis\n",
    "ax0 = plt.subplot2grid((2, 3), (0, 0), colspan=3, fig=fig)\n",
    "ax1 = plt.subplot2grid((2, 3), (1, 0), colspan=1, fig=fig)\n",
    "ax2 = plt.subplot2grid((2, 3), (1, 1), colspan=1, fig=fig)\n",
    "ax3 = plt.subplot2grid((2, 3), (1, 2), colspan=1, fig=fig)\n",
    "\n",
    "# All twits plot\n",
    "wfreq_all = collections.Counter(txt_all.split()).most_common(30)\n",
    "words, freqs = zip(*wfreq_all)\n",
    "ax0.bar(words, freqs)\n",
    "ax0.set_xticklabels(words, rotation=45)\n",
    "ax0.set_title(\"All Twits Top 30 Frequent Words\", fontsize=20)\n",
    "\n",
    "# Bearish twits plot\n",
    "wfreq_bearish = collections.Counter(txt_bearish.split()).most_common(10)\n",
    "words, freqs = zip(*wfreq_bearish)\n",
    "ax1.bar(words, freqs)\n",
    "ax1.set_xticklabels(words, rotation=45)\n",
    "ax1.set_title(\"Bearish Twits Top 10 Frequent Words\", fontsize=20)\n",
    "\n",
    "# Bullish twits plot\n",
    "wfreq_bullish = collections.Counter(txt_bullish.split()).most_common(10)\n",
    "words, freqs = zip(*wfreq_bullish)\n",
    "ax2.bar(words, freqs)\n",
    "ax2.set_xticklabels(words, rotation=45)\n",
    "ax2.set_title(\"Bullish Twits Top 10 Frequent Words\", fontsize=20)\n",
    "\n",
    "# Non-labeld twits plot\n",
    "wfreq_nan = collections.Counter(txt_nan.split()).most_common(10)\n",
    "words, freqs = zip(*wfreq_nan)\n",
    "ax3.bar(words, freqs)\n",
    "ax3.set_xticklabels(words, rotation=45)\n",
    "ax3.set_title(\"Non-Labeled Twits Top 10 Frequent Words\", fontsize=20)\n",
    "\n",
    "plt.savefig(\"./imgs/top_frequent_words.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of Words per Twit Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(21, 15))\n",
    "\n",
    "# Sets the axis\n",
    "ax0 = plt.subplot2grid((2, 3), (0, 0), colspan=3, fig=fig)\n",
    "ax1 = plt.subplot2grid((2, 3), (1, 0), colspan=1, fig=fig)\n",
    "ax2 = plt.subplot2grid((2, 3), (1, 1), colspan=1, fig=fig)\n",
    "ax3 = plt.subplot2grid((2, 3), (1, 2), colspan=1, fig=fig)\n",
    "\n",
    "# All twits plot\n",
    "wcount_all = df['text_heavy_clean'].str.split().str.len()\n",
    "sns.histplot(wcount_all, bins=100, kde=True, ax=ax0)\n",
    "ax0.set_title(\"All Twits Word Count Distribution\")\n",
    "ax0.set_xlabel(\"Twit Word Count\")\n",
    "\n",
    "# Bearish twits plot\n",
    "wcount_bearish = df[df['label'] == 'Bearish']['text_heavy_clean'].str.split().str.len()\n",
    "sns.histplot(wcount_bearish, bins=100, kde=True, ax=ax1)\n",
    "ax1.set_title(\"Bearish Twits Word Count Distribution\")\n",
    "ax1.set_xlabel(\"Twit Word Count\")\n",
    "\n",
    "# Bullish twits plot\n",
    "wcount_bullish = df[df['label'] == 'Bullish']['text_heavy_clean'].str.split().str.len()\n",
    "sns.histplot(wcount_bullish, bins=100, kde=True, ax=ax2)\n",
    "ax2.set_title(\"Bullish Twits Word Count Distribution\")\n",
    "ax2.set_xlabel(\"Twit Word Count\")\n",
    "\n",
    "# Non-labeld twits plot\n",
    "wcount_nan = df[df['label'].isna()]['text_heavy_clean'].str.split().str.len()\n",
    "sns.histplot(wcount_nan, bins=100, kde=True, ax=ax3)\n",
    "ax3.set_title(\"Non-Labeled Twits Word Count Distribution\")\n",
    "ax3.set_xlabel(\"Twit Word Count\")\n",
    "\n",
    "plt.savefig(\"./imgs/number_of_words_per_tweet_distribution.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Balanced Data Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selects only useful columns\n",
    "df_small = df[['id', 'user.type', 'base_asset', 'text', 'label']].dropna()\n",
    "\n",
    "# Drop duplicates on text (same twit can tag multiple sabe_asset)\n",
    "df_small.drop_duplicates('text', ignore_index=True, inplace=True)\n",
    "\n",
    "# Bots removal\n",
    "df_small = df_small[df_small['user.type'] == 'User']\n",
    "\n",
    "# Gets a small sample of the dataset for training and testing (balanced labels and base_assets)\n",
    "df_small = df_small.groupby('base_asset', group_keys=False).apply(lambda x: x.groupby('label', group_keys=False).apply(lambda y: y.sample(x['label'].value_counts().min())))\n",
    "\n",
    "# Resets the index\n",
    "df_small.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Test Validation Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracts the explanatory and explanable variables\n",
    "X = df_small[['text', 'text_light_clean', 'text_heavy_clean']]\n",
    "y = df_small['label'].replace({'Bearish': 0, 'Bullish': 1})\n",
    "\n",
    "# Splits the data into train, test and validation sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, random_state=1)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, train_size=0.9, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT Based Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sets the input of the Neural Network\n",
    "text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')\n",
    "\n",
    "# Obtains the output of the Neural Network\n",
    "output = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\", name='preprocessing')(text_input)\n",
    "output = hub.KerasLayer(\"https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-128_A-2/2\", trainable=True, name='BERT_encoder')(output)\n",
    "output = tf.keras.layers.Dropout(0.6)(output['sequence_output'])\n",
    "output = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128, return_sequences=True))(output)\n",
    "output = tf.keras.layers.Attention(name='attention')([output, output])\n",
    "output = tf.keras.layers.Conv1D(128, 9, activation='relu', padding='same', name='convolutional')(output)\n",
    "output = tf.keras.layers.GlobalAveragePooling1D(name='average_pooling')(output)\n",
    "output = tf.keras.layers.Dropout(0.4)(output)\n",
    "output = tf.keras.layers.Dense(1, activation='sigmoid', name='classifier')(output)\n",
    "\n",
    "# Defines the optimizer of the Neural Network\n",
    "learning_rate = 1e-4\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "# Defines the loss function of the Neural Network\n",
    "loss = 'binary_crossentropy'\n",
    "\n",
    "# Builds and compiles the model\n",
    "model = tf.keras.Model(inputs=text_input, outputs=output)\n",
    "model.compile(optimizer, loss=loss, metrics=['accuracy', tf.keras.metrics.AUC(name='auc')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sets training params\n",
    "epochs = 30\n",
    "batch_size = 512\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.ModelCheckpoint(\"./models/bert/weights.h5\", monitor='val_accuracy', save_freq='epoch', save_best_only=True),\n",
    "    tf.keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0.0005, patience=5, restore_best_weights=True),\n",
    "    tf.keras.callbacks.CSVLogger(\"./models/bert/history.csv\", separator=',', append=True)\n",
    "]\n",
    "\n",
    "# Trains the model\n",
    "history = model.fit(\n",
    "    x=X_train['text'], y=y_train,\n",
    "    validation_data=(X_val['text'], y_val),\n",
    "    epochs=epochs,\n",
    "    batch_size=batch_size,\n",
    "    callbacks=callbacks,\n",
    ")\n",
    "\n",
    "# Saves the model\n",
    "model.save(\"./models/bert\")\n",
    "\n",
    "# Loads the saved variables\n",
    "model = tf.keras.models.load_model(\"./models/bert\")\n",
    "history = pd.read_csv(\"./models/bert/history.csv\", index_col=0)\n",
    "\n",
    "# Display the history\n",
    "clear_output()\n",
    "history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtains the model label predictions for the test set\n",
    "y_test_scores = pd.Series(model.predict(X_test['text']).flatten())\n",
    "y_test_pred = y_test_scores.round()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Metrics plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, axs = plt.subplots(1, 3, figsize=(21, 7))\n",
    "\n",
    "axs[0].plot(history['loss'], label='Train')\n",
    "axs[0].plot(history['val_loss'], label='Validation')\n",
    "axs[0].set_title(\"Loss Value\")\n",
    "axs[0].set_ylabel(\"Loss Value\")\n",
    "axs[0].set_xlabel(\"Epoch\")\n",
    "axs[0].legend()\n",
    "\n",
    "axs[1].plot(history['accuracy'], label='Train')\n",
    "axs[1].plot(history['val_accuracy'], label='Validation')\n",
    "axs[1].set_title(\"Accuracy\")\n",
    "axs[1].set_ylabel(\"Accuracy\")\n",
    "axs[1].set_xlabel(\"Epoch\")\n",
    "axs[1].legend()\n",
    "\n",
    "axs[2].plot(history['auc'], label='Train')\n",
    "axs[2].plot(history['val_auc'], label='Validation')\n",
    "axs[2].set_title(\"AUC\")\n",
    "axs[2].set_ylabel(\"AUC\")\n",
    "axs[2].set_xlabel(\"Epoch\")\n",
    "axs[2].legend()\n",
    "\n",
    "plt.savefig(\"./imgs/bert_training_metrics_plot.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtains the confusion matrix\n",
    "cf_matrix = confusion_matrix(y_test, y_test_pred)\n",
    "\n",
    "# Obtains the annotations\n",
    "counts = [ f\"{val:0.0f}\" for val in cf_matrix.flatten() ]\n",
    "pcts = [ f\"{100*val:.2f}\" for val in cf_matrix.flatten()/np.sum(cf_matrix) ]\n",
    "annot = np.asarray([ f\"{count}\\n({pct}%)\" for count, pct in zip(counts, pcts) ]).reshape(2, 2)\n",
    "\n",
    "# Plots the confusion matrix\n",
    "ax = sns.heatmap(cf_matrix, annot=annot, cmap='Blues', fmt='')\n",
    "ax.set_title(\"BERT Model Confusion Matrix\")\n",
    "ax.set_ylabel(\"True\", fontsize=20)\n",
    "ax.set_xlabel(\"Predicted\", fontsize=20)\n",
    "ax.xaxis.set_ticklabels(['Bearish', 'Bullish'], fontsize=10) \n",
    "ax.yaxis.set_ticklabels(['Bearish', 'Bullish'], fontsize=10)\n",
    "plt.savefig(\"./imgs/bert_confusion_matrix.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ROC Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get true positive rates and false positive rates\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_test_scores)\n",
    "\n",
    "# Plots the ROC curve\n",
    "plt.plot(fpr, tpr)\n",
    "plt.plot([0, 1], [0, 1], linestyle=\"--\")\n",
    "\n",
    "# Shades the AUC and show its value\n",
    "filled_part = plt.fill_between(fpr, tpr, color='#8EB9D7')\n",
    "(x0, y0), (x1, y1) = filled_part.get_paths()[0].get_extents().get_points()\n",
    "plt.text(x1/2, y1/3, f\"AUC = {roc_auc_score(y_test, y_test_scores):.3f}\", fontsize=16)\n",
    "\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"BERT Receiver Operating Characteristic (ROC)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtains the classification report as a dataframe\n",
    "df_clf_report = pd.DataFrame(classification_report(y_test, y_test_pred, output_dict=True)).rename(columns={'0': 'Bearish', '1': 'Bullish'}).transpose().reset_index().rename(columns={ 'index': ''})\n",
    "\n",
    "# Plots the classification report\n",
    "fig, ax = render_df_table(df_clf_report, header_columns=1)\n",
    "fig.savefig(\"./imgs/bert_classification_report.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERTweet Based Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sets the input of the Neural Network\n",
    "text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')\n",
    "\n",
    "# Obtains the output of the Neural Network\n",
    "output = preprocessing(text_input)\n",
    "output = encoder(output['input_word_ids'], attention_mask=output['input_mask'], token_type_ids=output['input_type_ids'])\n",
    "output = tf.keras.layers.Dropout(0.6)(output['last_hidden_state'])\n",
    "output = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128, return_sequences=True))(output)\n",
    "output = tf.keras.layers.Attention(name='attention')([output, output])\n",
    "output = tf.keras.layers.Conv1D(128, 9, activation='relu', padding='same', name='convolutional')(output)\n",
    "output = tf.keras.layers.GlobalAveragePooling1D(name='average_pooling')(output)\n",
    "output = tf.keras.layers.Dropout(0.4)(output)\n",
    "output = tf.keras.layers.Dense(1, activation='sigmoid', name='classifier')(output)\n",
    "\n",
    "# Defines the optimizer of the Neural Network\n",
    "learning_rate = 1e-4\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "# Defines the loss function of the Neural Network\n",
    "loss = 'binary_crossentropy'\n",
    "\n",
    "# Builds and compiles the model\n",
    "model = tf.keras.Model(inputs=text_input, outputs=output)\n",
    "model.compile(optimizer, loss=loss, metrics=['accuracy', tf.keras.metrics.AUC(name='auc')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Whole Dataset Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_twits = pd.read_csv(\"./datasets/classified/twits.csv.gz\", index_col=0, parse_dates=['date'], low_memory=False)\n",
    "df_twits.dropna(subset=['label'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = df_twits['label'].replace({ 'Bearish': 0, 'Bullish': 1 })\n",
    "y_test_scores = df_twits['label_pred_score']\n",
    "y_test_pred = df_twits['label_pred'].replace({ 'Bearish': 0, 'Bullish': 1 })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtains the confusion matrix\n",
    "cf_matrix = confusion_matrix(y_test, y_test_pred)\n",
    "\n",
    "# Obtains the annotations\n",
    "counts = [ f\"{val:0.0f}\" for val in cf_matrix.flatten() ]\n",
    "pcts = [ f\"{100*val:.2f}\" for val in cf_matrix.flatten()/np.sum(cf_matrix) ]\n",
    "annot = np.asarray([ f\"{count}\\n({pct}%)\" for count, pct in zip(counts, pcts) ]).reshape(2, 2)\n",
    "\n",
    "# Plots the confusion matrix\n",
    "ax = sns.heatmap(cf_matrix, annot=annot, cmap='Blues', fmt='')\n",
    "ax.set_title(\"BERT Model Confusion Matrix\")\n",
    "ax.set_ylabel(\"True\", fontsize=20)\n",
    "ax.set_xlabel(\"Predicted\", fontsize=20)\n",
    "ax.xaxis.set_ticklabels(['Bearish', 'Bullish'], fontsize=10) \n",
    "ax.yaxis.set_ticklabels(['Bearish', 'Bullish'], fontsize=10)\n",
    "plt.savefig(\"./imgs/bert_confusion_matrix_full.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ROC Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get true positive rates and false positive rates\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_test_scores)\n",
    "\n",
    "# Plots the ROC curve\n",
    "plt.plot(fpr, tpr)\n",
    "plt.plot([0, 1], [0, 1], linestyle=\"--\")\n",
    "\n",
    "# Shades the AUC and show its value\n",
    "filled_part = plt.fill_between(fpr, tpr, color='#8EB9D7')\n",
    "(x0, y0), (x1, y1) = filled_part.get_paths()[0].get_extents().get_points()\n",
    "plt.text(x1/2, y1/3, f\"AUC = {roc_auc_score(y_test, y_test_scores):.3f}\", fontsize=16)\n",
    "\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"BERT Receiver Operating Characteristic (ROC)\")\n",
    "plt.savefig(\"./imgs/bert_roc_full.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtains the classification report as a dataframe\n",
    "df_clf_report = pd.DataFrame(classification_report(y_test, y_test_pred, output_dict=True)).rename(columns={'0': 'Bearish', '1': 'Bullish'}).transpose().reset_index().rename(columns={ 'index': ''})\n",
    "\n",
    "# Plots the classification report\n",
    "fig, ax = render_df_table(df_clf_report, header_columns=1)\n",
    "fig.savefig(\"./imgs/bert_classification_report_full.png\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
